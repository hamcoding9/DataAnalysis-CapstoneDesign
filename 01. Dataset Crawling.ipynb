{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65dbabf9",
   "metadata": {},
   "source": [
    "# 산문시\n",
    "* 정의\n",
    "\n",
    "    * 행과 연의 구분이 없는 시\n",
    "    \n",
    "    * 구체적으로 정의하자면, 짧고 압축된 형태로 행과 연을 파괴한 줄글이며 상상, 리듬, 표현의 밀도 등 시적 장치를 지닌 하나의 예술 형태이다.\n",
    "\n",
    "\n",
    "\n",
    "* 산문시 vs 운문시 vs 산문\n",
    "\n",
    "<img src=\"산문시.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "* 이 프로젝트에서 **산문시**란, **한 단락**으로 되어 있으며 **시를 미적으로 만드는 형식(운율, 함축, 모호성, 이미지)**을 갖추고 있는 문학의 형태이다.\n",
    "\n",
    "* 따라서 산문시의 형태를 취하고 있더라도 미적인 요소가 없으면 데이터에서 제외시킨다.\n",
    "\n",
    "* 도움되는 글\n",
    "    * [산문시에 관하여(평론)](https://teen.munjang.or.kr/archives/14886)\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "* Transfer Learning 방식을 사용할 것이므로 **상대적으로 양이 적어질지라도 데이터의 품질에 집중한다!**\n",
    "\n",
    "* 어디서 크롤링할 것인가?\n",
    "    * 문학광장 글틴: https://teen.munjang.or.kr/archives/category/write/poetry\n",
    "    * 포스타입 <산문시> 태그: https://www.postype.com/search?keyword=%EC%82%B0%EB%AC%B8%EC%8B%9C&options_tags=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b9e58",
   "metadata": {},
   "source": [
    "# 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a0d01",
   "metadata": {},
   "source": [
    "### 1. 포스타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ea7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# selenium 로그 숨기기\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "# 크롬 브라우저 화면 띄우지 않기\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "de9cc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url_lists(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    posts = soup.find_all(\"div\", attrs = {\"class\":\"post-media-body\"})\n",
    "    for post in posts:\n",
    "        post_url = post.find(\"a\", attrs = {\"class\": \"post-title-wrap ellipsis-2\"})['href']\n",
    "        url_list.append(post_url)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c6bc368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 긁어오기\n",
    "driver = webdriver.Chrome(options = options)\n",
    "\n",
    "def scraping(url_list, pList):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        for url in url_list:\n",
    "            driver.get(url)\n",
    "            title = driver.find_element(By.XPATH, \"//*[@id='post']/div/header/div[1]/h1\")\n",
    "            contents = driver.find_element(By.XPATH, \"//*[@id='post-content']\")\n",
    "            pList.append([title.text, contents.text])\n",
    "    except NoSuchElementException:\n",
    "            print(\"No element exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cd8254f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def saveToFile(filename, pList):\n",
    "    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(pList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cdf90ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pList = []\n",
    "for i in range(1, 37):\n",
    "    url_list = []\n",
    "    url = \"https://www.postype.com/search?page=\"+str(i)+\"&keyword=%EC%82%B0%EB%AC%B8%EC%8B%9C&options_tags=1\" # 1 page 선택\n",
    "    url_list = make_url_lists(url) # 1 page에 있는 모든 포스트들의 url을 list로 받음\n",
    "    scraping(url_list, pList)\n",
    "saveToFile(\"ptype.csv\", pList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96eccb6",
   "metadata": {},
   "source": [
    "### 2. 글틴 - 7725건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5415b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# selenium 로그 숨기기\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "# 크롬 브라우저 화면 띄우지 않기\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed2bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url_lists(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    posts = soup.find_all(\"div\", attrs = {\"class\": \"entry-content\"}) \n",
    "    for post in posts:\n",
    "        content = post.find(\"div\", attrs = {\"class\" : \"post_title\"})\n",
    "        title = content.text\n",
    "        if '공지' in title:\n",
    "            continue\n",
    "        if '장원' in title:\n",
    "            continue\n",
    "        url = content.find(\"a\")['href']\n",
    "        url_list.append(url)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb63ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 긁어오기\n",
    "def scraping(url_list, pList):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        for url in url_list:\n",
    "            driver.get(url)\n",
    "            title = driver.find_element(By.CLASS_NAME, \"entry-title\")\n",
    "            contents = driver.find_element(By.CLASS_NAME, \"entry-content\")\n",
    "            pList.append([title.text, contents.text])\n",
    "    except NoSuchElementException:\n",
    "            print(\"No element exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7957d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(filename, pList):\n",
    "    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(pList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e593fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "약 55.0분 소요\n"
     ]
    }
   ],
   "source": [
    "pList = []\n",
    "start = time.time()\n",
    "for i in range(1, 400):\n",
    "    url_list = []\n",
    "    url = \"https://teen.munjang.or.kr/archives/category/write/poetry/page/\" + str(i) # 1 page 선택\n",
    "    url_list = make_url_lists(url) # 1 page에 있는 모든 포스트들의 url을 list로 받음\n",
    "    scraping(url_list, pList)\n",
    "saveToFile(\"teenpage501-800.csv\", pList) \n",
    "end = time.time()\n",
    "print(\"약 {}분 소요\".format((end - start)//60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
